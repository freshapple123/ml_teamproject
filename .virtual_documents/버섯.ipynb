


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

mushrooms_df = pd.read_csv('mushrooms.csv')
mushrooms_df.head(3)


print('\n ### train Îç∞Ïù¥ÌÑ∞ Ï†ïÎ≥¥ ###  \n')
print(mushrooms_df.info())


# Í∞Å Ïª¨ÎüºÎ≥Ñ Í≤∞Ï∏°Ïπò Í∞úÏàò Ï∂úÎ†•
print(mushrooms_df.isnull().sum())


print("Ï†ÑÏ≤¥ Null Í∞í Í∞úÏàò:", mushrooms_df.isnull().sum().sum())





from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

def print_clf_eval(y_test, pred, model_name=''):
    print(f'\nüìä {model_name} ÏÑ±Îä• ÌèâÍ∞Ä Í≤∞Í≥º')
    print('Ïò§Ï∞® ÌñâÎ†¨:\n', confusion_matrix(y_test, pred))
    print('Ï†ïÌôïÎèÑ (Accuracy): {:.4f}'.format(accuracy_score(y_test, pred)))
    print('Ï†ïÎ∞ÄÎèÑ (Precision): {:.4f}'.format(precision_score(y_test, pred)))
    print('Ïû¨ÌòÑÏú® (Recall): {:.4f}'.format(recall_score(y_test, pred)))
    print('F1 Ï†êÏàò (F1 Score): {:.4f}'.format(f1_score(y_test, pred)))



# Î¨∏ÏûêÌòï ‚Üí Ïà´ÏûêÌòï Î≥ÄÌôò (Label Encoding)
label_encoder = LabelEncoder()
for col in mushrooms_df.columns:
    mushrooms_df[col] = label_encoder.fit_transform(mushrooms_df[col])

# Îç∞Ïù¥ÌÑ∞/Î†àÏù¥Î∏î Î∂ÑÎ¶¨
X = mushrooms_df.drop("class", axis=1)
y = mushrooms_df["class"]

# ÌïôÏäµ/ÌÖåÏä§Ìä∏ Î∂ÑÌï†
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)

# Î™®Îç∏ Ï†ïÏùò
dt_clf = DecisionTreeClassifier(random_state=11)
rf_clf = RandomForestClassifier(random_state=11)
lr_clf = LogisticRegression(solver='liblinear')  # ÏûëÏùÄ Îç∞Ïù¥ÌÑ∞Ïóê Ï†ÅÌï©

# Í≤∞Ï†ï Ìä∏Î¶¨ ÌïôÏäµ/ÏòàÏ∏°
dt_clf.fit(X_train, y_train)
dt_pred = dt_clf.predict(X_test)
print('DecisionTreeClassifier Ï†ïÌôïÎèÑ: {0:.4f}'.format(accuracy_score(y_test, dt_pred)))

# ÎûúÎç§ Ìè¨Î†àÏä§Ìä∏ ÌïôÏäµ/ÏòàÏ∏°
rf_clf.fit(X_train, y_train)
rf_pred = rf_clf.predict(X_test)
print('RandomForestClassifier Ï†ïÌôïÎèÑ: {0:.4f}'.format(accuracy_score(y_test, rf_pred)))

# Î°úÏßÄÏä§Ìã± ÌöåÍ∑Ä ÌïôÏäµ/ÏòàÏ∏°
lr_clf.fit(X_train, y_train)
lr_pred = lr_clf.predict(X_test)
print('LogisticRegression Ï†ïÌôïÎèÑ: {0:.4f}'.format(accuracy_score(y_test, lr_pred)))



print_clf_eval(y_test, dt_pred, 'DecisionTreeClassifier')
print_clf_eval(y_test, rf_pred, 'RandomForestClassifier')
print_clf_eval(y_test, lr_pred, 'LogisticRegression')






from sklearn.tree import export_graphviz
import graphviz

# class_namesÎäî 0 = edible, 1 = poisonous
class_names = ['edible', 'poisonous']

# feature_namesÎäî ÌïôÏäµ Îç∞Ïù¥ÌÑ∞Ïùò Ïª¨Îüº Î¶¨Ïä§Ìä∏
feature_names = X.columns.tolist()

# DOT ÌååÏùº ÏÉùÏÑ±
export_graphviz(dt_clf, out_file="mushroom.dot",
                class_names=class_names,
                feature_names=feature_names,
                impurity=True, filled=True)

# DOT ÌååÏùºÏùÑ Î∞îÎ°ú Í∑∏ÎûòÌîÑÎ°ú Ï∂úÎ†•
with open("mushroom.dot") as f:
    dot_graph = f.read()

# Jupyter ÌôòÍ≤ΩÏóêÏÑú ÏãúÍ∞ÅÌôî
graphviz.Source(dot_graph)






import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import xgboost as xgb
import numpy as np

# Îç∞Ïù¥ÌÑ∞ Î°úÎî©
mushrooms_df = pd.read_csv("mushrooms.csv")

# ÎùºÎ≤® Ïù∏ÏΩîÎî© (Î™®Îì† Ïª¨ÎüºÏù¥ Î¨∏ÏûêÏó¥Ïù¥Îùº Ï†ÑÎ∂Ä Ïù∏ÏΩîÎî© ÌïÑÏöî)
label_encoder = LabelEncoder()
for col in mushrooms_df.columns:
    mushrooms_df[col] = label_encoder.fit_transform(mushrooms_df[col])

# Feature / Label ÎÇòÎàÑÍ∏∞
X_features = mushrooms_df.iloc[:, 1:]  # Ï≤´ Ïª¨Îüº 'class'Í∞Ä label
y_label = mushrooms_df.iloc[:, 0]      # 'class' Ïª¨Îüº





# ÌïôÏäµ Îç∞Ïù¥ÌÑ∞ -> ÌïôÏäµ/Í≤ÄÏ¶ù
X_tr, X_val, y_tr, y_val = train_test_split(
    X_train, y_train, test_size=0.1, random_state=156)

print(X_train.shape, X_test.shape)
print(X_tr.shape, X_val.shape)



# DMatrix ÏÉùÏÑ±
dtr = xgb.DMatrix(data=X_tr, label=y_tr)
dval = xgb.DMatrix(data=X_val, label=y_val)
dtest = xgb.DMatrix(data=X_test, label=y_test)

# ÌååÎùºÎØ∏ÌÑ∞ ÏÑ§Ï†ï
params = {
    'max_depth': 3,
    'eta': 0.05,
    'objective': 'binary:logistic',
    'eval_metric': 'logloss'
}
num_rounds = 400
eval_list = [(dtr, 'train'), (dval, 'eval')]

# ÌïôÏäµ
xgb_model = xgb.train(params=params, dtrain=dtr, num_boost_round=num_rounds,
                      early_stopping_rounds=50, evals=eval_list)



# ÏòàÏ∏° ÌôïÎ•†
pred_probs = xgb_model.predict(dtest)
print('ÏòàÏ∏° ÌôïÎ•† Í∞í 10Í∞ú:', np.round(pred_probs[:10], 3))

# ÌôïÎ•† -> Ïù¥ÏßÑ ÌÅ¥ÎûòÏä§(0 ÎòêÎäî 1)
preds = [1 if x > 0.5 else 0 for x in pred_probs]
print('ÏòàÏ∏° ÌÅ¥ÎûòÏä§ 10Í∞ú:', preds[:10])



from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def get_clf_eval(y_test, pred=None, pred_proba=None):
    confusion = confusion_matrix(y_test, pred)
    accuracy = accuracy_score(y_test, pred)
    precision = precision_score(y_test, pred)
    recall = recall_score(y_test, pred)
    f1 = f1_score(y_test, pred)
    roc_auc = roc_auc_score(y_test, pred_proba)

    print('Ïò§Ï∞® ÌñâÎ†¨')
    print(confusion)
    print('Ï†ïÌôïÎèÑ: {0:.4f}, Ï†ïÎ∞ÄÎèÑ: {1:.4f}, Ïû¨ÌòÑÏú®: {2:.4f},\
    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))

get_clf_eval(y_test, preds, pred_probs)



models = ['DecisionTree', 'RandomForest', 'LogisticRegression', 'XGBoost']
accuracy_list = []
precision_list = []
recall_list = []
f1_list = []
auc_list = []

from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score

def get_metrics(y_true, y_pred, y_proba):
    return {
        'accuracy': accuracy_score(y_true, y_pred),
        'precision': precision_score(y_true, y_pred),
        'recall': recall_score(y_true, y_pred),
        'f1': f1_score(y_true, y_pred),
        'auc': roc_auc_score(y_true, y_proba)
    }

# Decision Tree
dt_metrics = get_metrics(y_test, dt_pred, dt_clf.predict_proba(X_test)[:,1])
# Random Forest
rf_metrics = get_metrics(y_test, rf_pred, rf_clf.predict_proba(X_test)[:,1])
# Logistic Regression
lr_metrics = get_metrics(y_test, lr_pred, lr_clf.predict_proba(X_test)[:,1])
# XGBoost
xgb_metrics = get_metrics(y_test, preds, pred_probs)

for m in [dt_metrics, rf_metrics, lr_metrics, xgb_metrics]:
    accuracy_list.append(m['accuracy'])
    precision_list.append(m['precision'])
    recall_list.append(m['recall'])
    f1_list.append(m['f1'])
    auc_list.append(m['auc'])



import matplotlib.pyplot as plt
import matplotlib as mpl
from matplotlib import font_manager, rc

# ÌïúÍ∏Ä Ìè∞Ìä∏ ÏÑ§Ï†ï (ÏúàÎèÑÏö∞ Í∏∞Î≥∏: Malgun Gothic)
font_path = "C:/Windows/Fonts/malgun.ttf"  # 'ÎßëÏùÄ Í≥†Îîï' Í≤ΩÎ°ú
font_name = font_manager.FontProperties(fname=font_path).get_name()
rc('font', family=font_name)

# ÎßàÏù¥ÎÑàÏä§ Í∏∞Ìò∏ Íπ®Ïßê Î∞©ÏßÄ
mpl.rcParams['axes.unicode_minus'] = False



import matplotlib.pyplot as plt
import numpy as np

bar_width = 0.15
index = np.arange(len(models))

plt.figure(figsize=(12, 6))
plt.bar(index, accuracy_list, bar_width, label='Accuracy')
plt.bar(index + bar_width, precision_list, bar_width, label='Precision')
plt.bar(index + bar_width*2, recall_list, bar_width, label='Recall')
plt.bar(index + bar_width*3, f1_list, bar_width, label='F1 Score')
plt.bar(index + bar_width*4, auc_list, bar_width, label='AUC')

plt.xticks(index + bar_width*2, models)
plt.ylim(0.3, 1.05)
plt.title('Î™®Îç∏Î≥Ñ Î∂ÑÎ•ò ÏÑ±Îä• ÎπÑÍµê')
plt.legend(loc='lower right')
plt.grid(axis='y')
plt.show()



importances = rf_clf.feature_importances_
features = X_features.columns

plt.figure(figsize=(10, 6))
plt.barh(range(len(importances)), importances, align='center')
plt.yticks(np.arange(len(importances)), features)
plt.xlabel('Feature Importance')
plt.title('Random Forest Feature Importance')
plt.show()



xgb.plot_importance(xgb_model, max_num_features=10, height=0.4)
plt.title("XGBoost Feature Importance")
plt.show()



from sklearn.metrics import roc_curve, auc

plt.figure(figsize=(10, 6))

# DecisionTree
fpr_dt, tpr_dt, _ = roc_curve(y_test, dt_clf.predict_proba(X_test)[:,1])
# RandomForest
fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_clf.predict_proba(X_test)[:,1])
# LogisticRegression
fpr_lr, tpr_lr, _ = roc_curve(y_test, lr_clf.predict_proba(X_test)[:,1])
# XGBoost
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, pred_probs)

plt.plot(fpr_dt, tpr_dt, label='Decision Tree')
plt.plot(fpr_rf, tpr_rf, label='Random Forest')
plt.plot(fpr_lr, tpr_lr, label='Logistic Regression')
plt.plot(fpr_xgb, tpr_xgb, label='XGBoost')

plt.plot([0,1],[0,1],'k--') # ÎåÄÍ∞ÅÏÑ†
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend(loc='lower right')
plt.grid()
plt.show()




